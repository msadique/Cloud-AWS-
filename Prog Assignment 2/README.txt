Total Number of Files 

1 output.pdf: Contains the output from different sort files.
2. pa2_report.pdf: Brief description of the problem, methodology, and runtime environment settings.
3. HadoopConfigurationFiles: All files of Hadoop configuration.
4. Scripts: All scripts used in the assignment.
5. Jars: contains all the jars of the assignment.
3. SourceCode.txt: Source code of the assignment.


Shared Memory, Hadoop and Spark Installation and execution Steps:


For Shared Memory:

1. Download Gensort from: http://www.ordinal.com/try.cgi/gensort-linux-1.5.tar.gz
2. Commands to extract the downloaded zip file:  $ tar -xvzf hadoop-2.7.2.tar.gz
3. Inside extracted folder 64, type command to generate dataset: $ ./gensort -a <file_size> <file_name>
4. Copy the Terasort jar files to root location along and also the file generated by the gensort application.: $ java -jar sharedmem.jar
5. The application will give the following options:
a. Input file name
b. Output file name:
c. Number of threads:
6. If successfully executed,output will be generated at the location of jar.
7. To verify the output file we have to use valsort that is in the folder "64/". Type the command: $ ./valsort <ouput_file_name>


For Hadoop:
1. First login to the master node. Using Winscp or scp tools copy the following scripts on the root location.
a. hadoop_install.sh
2. When uploaded, run the following commands:
$ sudo chmod 777 hadoop_install.sh
3. Now run the following command. 
$ ./hadoop_install.sh
when prompted for input enter 'y' or 'yes'
4. When successfully executed, the Hadoop and Spark environment will be setup on the instance.
5. Now generate the public/private RSA keys .Exxecute the following command: $ ssh-keygen
6. The key will be generated and stored. Now copy the generated key to the “authorized_keys”.
7. Now create a config file in the folder "~/.ssh/" and add the following line StrictHostKeyCheck=no
And change the permissions to 600 using chmod.
8. Now update configuration files of hadoop based on the single-node and multi-node cluster setup.
a. mapred-site.xml
b. yarn-site.xml
c. core-site.xml
d. hdfs-site.xml
e. masters
f. slaves
9. Using winscp or scp tools copy the Hadooop Terasort jar files on to the EBS mounted volume. 
then copy the gensort file from 64 folder to the /mnt/raid/.
Run the command for generating the data file: $ ./gensort -a <file_size> <file_name>
10. Now run the $ hdfs dfs namenode -format 
11. Start the master and cluster nodes: $ start-dfs.sh
12. Start resource manager: $ start-yarn.sh
13. To Check if all the processess are working: $ jps
14. Copy the input data file to HDFS: $ hdfs dfs -copyFromLocal <input_file_location> /input
15. Run the application:$ hadoop jar hterasort.jar <input_file_name> /output
16. Copy the output data file from HDFS to local:$ hdfs dfs -copyToLocal /ouput <ouput_file_location>
17. To check the output type: $ ./valsort <file_name>
18. To Stop services type : $ stop-dfs.sh , $ stop-yarn.sh


Spark:
1. If installation steps followed in Hadoop is succesfully executed then skip to 2nd step else go through  
Steps 1 to 6 in the Hadoop installation instructions.
2. Create a temperory folder if it not there.
3. Using winscp tools copy the Spark Terasort jar files on to the EBS mounted volume. and copy the gensort file to the root.
Using the following command make the data file.
$ ./gensort -a <file_size> <file_name>
4. The modification of configuration files has been explained in the pa2_report. 
5. when modification completes then start the spark master by typing 
$ start-master.sh
6. Also to Start the slave 
$ start-slave.sh
7. Then run the command to copy the input data file to HDFS:
$ hdfs dfs -copyFromLocal <input_file_location> /input
8. Run spark application at the folder using the following command:
$ ./spark-submit --class <application_main_class_name> 
--master local[*] <location_of_jar> 
9. when execution completes successfully 
$ hdfs dfs -copyToLocal /ouput <ouput_file_location>
10.Stop master and slave 
$ ./stop-master.sh
$ ./stop-slave.sh


